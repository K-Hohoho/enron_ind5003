{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IND5003 Enron Project\n",
    "\n",
    "### Dataset\n",
    "Dataset from Prof: \n",
    "(https://www.cs.cmu.edu/~./enron/)\n",
    "- Unstructured Dataset containing raw text in the form of emails\n",
    "**Make sure that the dataset 'maildir' is in the same directory as your project on your own system. Else this would not work**\n",
    "\n",
    "## Contents of this Jupyter Notebook\n",
    "\n",
    "### Section 1: Data Preprocessing\n",
    "- Import, Process and Clean Data\n",
    "\n",
    "    #### Steps for Data Preprocessing:\n",
    "    1. Generate `pandas` dataframe from the raw files in maildir\n",
    "    2. Clean Text\n",
    "    3. Stem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries required for Section 1\n",
    "import os # Required for directory traversal\n",
    "import pandas as pd\n",
    "import email\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maildir path to the respective paths in your system \n",
    "# ! Note that maildir should be in the same directory as your project on your own system, would change if you are using windows\n",
    "maildir_path = '/Users/Dylan/Documents/IND5003/Projects/maildir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arnold-j', 'phanis-s', 'lavorato-j', 'stclair-c', 'townsend-j', 'forney-j', 'symes-k', 'reitmeyer-j', 'hyatt-k', 'steffes-j', 'kaminski-v', 'wolfe-j', 'mcconnell-m', 'skilling-j', 'zipper-a', 'shively-h', 'donoho-l', 'sanchez-m', 'delainey-d', 'germany-c', 'whalley-l', 'buy-r', 'harris-s', 'tholt-j', 'cash-m', 'sanders-r', '.DS_Store', 'staab-t', 'semperger-c', 'mccarty-d', 'mclaughlin-e', 'ring-a', 'stokley-c', 'hain-m', 'weldon-c', 'ring-r', 'farmer-d', 'sager-e', 'zufferli-j', 'ybarbo-p', 'watson-k', 'dasovich-j', 'arora-h', 'slinger-r', 'martin-t', 'storey-g', 'ruscitti-k', 'shankman-j', 'schwieger-j', 'perlingiere-d', 'saibi-e', 'griffith-j', 'meyers-a', 'grigsby-m', 'taylor-m', 'rapp-b', 'causholli-m', 'derrick-j', 'bass-e', 'south-s', 'salisbury-h', 'beck-s', 'tycholiz-b', 'shackleton-s', 'kitchen-l', 'merriss-s', 'blair-l', 'quenet-j', 'lokey-t', 'williams-j', 'panus-s', 'gang-l', 'hendrickson-s', 'schoolcraft-d', 'mann-k', 'kuykendall-t', 'allen-p', 'giron-d', 'lewis-a', 'jones-t', 'carson-m', 'stepenovitch-j', 'whitt-m', 'love-p', 'whalley-g', 'presto-k', 'scott-s', 'crandell-s', 'rodrique-r', 'white-s', 'motley-m', 'sturm-f', 'dean-c', 'keiser-k', 'shapiro-r', 'corman-s', 'pereira-s', 'campbell-l', 'richey-c', 'ward-k', 'dickson-s', 'rogers-b', 'nemec-g', 'hayslett-r', 'haedicke-m', 'mckay-j', 'gay-r', 'brawner-s', 'lucci-p', 'king-j', 'geaccone-t', 'guzman-m', 'mckay-b', 'hyvl-d', 'williams-w3', 'davis-d', 'thomas-p', 'linder-e', 'heard-m', 'hodge-j', 'pimenov-v', 'neal-s', 'fossum-d', 'baughman-d', 'smith-m', 'mims-thurston-p', 'bailey-s', 'fischer-m', 'dorland-c', 'ermis-f', 'may-l', 'platter-p', 'keavey-p', 'cuilla-m', 'parks-j', 'lokay-m', 'kean-s', 'quigley-d', 'horton-s', 'benson-r', 'solberg-g', 'badeer-r', 'lay-k', 'gilbertsmith-d', 'lenhart-m', 'swerzbin-m', 'hernandez-j', 'holst-k', 'maggi-m', 'donohoe-t', 'scholtes-d']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the directories in the maildir for sanity check\n",
    "maildir_list = os.listdir(maildir_path)\n",
    "print(maildir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_email(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             msg = BytesParser(policy=policy.default).parse(f)\n",
    "        \n",
    "#         # Extract fields from the email\n",
    "#         email_from = msg['From']\n",
    "#         email_to = msg['To']\n",
    "#         email_date = msg['Date']\n",
    "#         email_subject = msg['Subject']\n",
    "#         email_body = msg.get_body(preferencelist=('plain')).get_content() if msg.get_body(preferencelist=('plain')) else ''\n",
    "        \n",
    "#         return [email_from, email_to, email_date, email_subject, email_body]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error parsing file {file_path}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def batch_iterator(iterator, batch_size):\n",
    "#     \"\"\"Yield batches of specified size from an iterator.\"\"\"\n",
    "#     while True:\n",
    "#         batch = list(islice(iterator, batch_size))\n",
    "#         if not batch:\n",
    "#             break\n",
    "#         yield batch\n",
    "\n",
    "# def load_emails(maildir_path, batch_size=10, max_emails=50):\n",
    "#     email_data = []\n",
    "#     file_paths = []\n",
    "\n",
    "#     # Walk through the directory to collect file paths\n",
    "#     for root, dirs, files in os.walk(maildir_path):\n",
    "#         for file in files:\n",
    "#             if file == '.DS_Store' or file.startswith('.'):\n",
    "#                 continue  # Skip system files and hidden files\n",
    "#             file_paths.append(os.path.join(root, file))\n",
    "#             if len(file_paths) >= max_emails:\n",
    "#                 break\n",
    "#         if len(file_paths) >= max_emails:\n",
    "#             break\n",
    "\n",
    "#     # Process emails in batches\n",
    "#     for batch in batch_iterator(iter(file_paths), batch_size):\n",
    "#         batch_data = []\n",
    "#         for file_path in batch:\n",
    "#             result = parse_email(file_path)\n",
    "#             if result is not None:\n",
    "#                 batch_data.append(result)\n",
    "        \n",
    "#         # Append batch data to the main list\n",
    "#         email_data.extend(batch_data)\n",
    "\n",
    "#     # Create a DataFrame from the extracted data\n",
    "#     df = pd.DataFrame(email_data, columns=['From', 'To', 'Date', 'Subject', 'Body'])\n",
    "#     return df\n",
    "\n",
    "# # Load and parse emails\n",
    "# emails_df = load_emails(maildir_path, batch_size=10, max_emails=50)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(emails_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data into a Pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/sent_items/24.: 'ValueTerminal' object does not support item assignment\n",
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/_americas/netco_eol/83.: 'ValueTerminal' object does not support item assignment\n",
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/_americas/netco_eol/82.: 'ValueTerminal' object does not support item assignment\n",
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/_americas/esvl/87.: 'ValueTerminal' object does not support item assignment\n",
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/_americas/netco_restart/3.: 'ValueTerminal' object does not support item assignment\n",
      "                        From  \\\n",
      "0            msagel@home.com   \n",
      "1    slafontaine@globalp.com   \n",
      "2    iceoperations@intcx.com   \n",
      "3  jeff.youngflesh@enron.com   \n",
      "4  caroline.abramo@enron.com   \n",
      "\n",
      "                                                  To  \\\n",
      "0                                  jarnold@enron.com   \n",
      "1                              john.arnold@enron.com   \n",
      "2  icehelpdesk@intcx.com, internalmarketing@intcx...   \n",
      "3  anthony.gilmore@enron.com, colleen.koenig@enro...   \n",
      "4                             mike.grigsby@enron.com   \n",
      "\n",
      "                              Date  \\\n",
      "0  Thu, 16 Nov 2000 09:30:00 -0800   \n",
      "1  Fri, 08 Dec 2000 05:05:00 -0800   \n",
      "2  Tue, 15 May 2001 09:43:00 -0700   \n",
      "3  Mon, 27 Nov 2000 01:49:00 -0800   \n",
      "4  Tue, 12 Dec 2000 09:33:00 -0800   \n",
      "\n",
      "                                             Subject  \\\n",
      "0                                             Status   \n",
      "1                                 re:summer inverses   \n",
      "2                      The WTI Bullet swap contracts   \n",
      "3  Invitation: EBS/GSS Meeting w/Bristol Babcock ...   \n",
      "4                                       Harvard Mgmt   \n",
      "\n",
      "                                                Body  \n",
      "0  John:\\n?\\nI'm not really sure what happened be...  \n",
      "1  i suck-hope youve made more money in natgas la...  \n",
      "2   Hi,\\n\\n\\n  Following the e-mail you have rece...  \n",
      "3  Conference Room TBD.  \\n\\nThis meeting will be...  \n",
      "4  Mike- I have their trader coming into the offi...  \n"
     ]
    }
   ],
   "source": [
    "def parse_email(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            msg = BytesParser(policy=policy.default).parse(f)\n",
    "        \n",
    "        # Extract fields from the email\n",
    "        email_from = msg['From']\n",
    "        email_to = msg['To']\n",
    "        email_date = msg['Date']\n",
    "        email_subject = msg['Subject']\n",
    "        email_body = msg.get_body(preferencelist=('plain')).get_content() if msg.get_body(preferencelist=('plain')) else ''\n",
    "        \n",
    "        return [email_from, email_to, email_date, email_subject, email_body]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_iterator(iterator, batch_size):\n",
    "    \"\"\"Yield batches of specified size from an iterator.\"\"\"\n",
    "    while True:\n",
    "        batch = list(islice(iterator, batch_size))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "def load_emails(maildir_path, batch_size=1000):\n",
    "    email_data = []\n",
    "    file_paths = []\n",
    "\n",
    "    # Walk through the directory to collect file paths\n",
    "    for root, dirs, files in os.walk(maildir_path):\n",
    "        for file in files:\n",
    "            if file == '.DS_Store' or file.startswith('.'):\n",
    "                continue  # Skip system files and hidden files\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    # Process emails in batches\n",
    "    for batch in batch_iterator(iter(file_paths), batch_size):\n",
    "        batch_data = []\n",
    "        for file_path in batch:\n",
    "            result = parse_email(file_path)\n",
    "            if result is not None:\n",
    "                batch_data.append(result)\n",
    "        \n",
    "        # Append batch data to the main list\n",
    "        email_data.extend(batch_data)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(email_data, columns=['From', 'To', 'Date', 'Subject', 'Body'])\n",
    "    return df\n",
    "\n",
    "# Load and parse emails\n",
    "emails_df = load_emails(maildir_path, batch_size=1000)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(emails_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emails_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert emails_df to a CSV file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Save the DataFrame as a CSV file in the specified directory\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43memails_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/Dylan/Documents/IND5003/Projects/emails_uncleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'emails_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert emails_df to a CSV file\n",
    "# Save the DataFrame as a CSV file in the specified directory\n",
    "\n",
    "#emails_df.to_csv('/Users/Dylan/Documents/IND5003/Projects/emails_uncleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file back into a DataFrame\n",
    "\n",
    "# PLEASE CHANGE IT TO YOUR OWN DIRECTORY IN YOUR OWN SYSTEM \n",
    "\n",
    "enron_uncleaned_emails = pd.read_csv('/Users/Dylan/Documents/IND5003/Projects/enron_emails_uncleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ind5003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')  
nltk.download('stopwords')  
#Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Define the cleaning and lemmatizing function
def clean_and_lemmatize(text):
    # Convert text to lowercase
    text = text.lower()
    
    # Remove email addresses, URLs, numbers, and special characters
    text = re.sub(r'\S*@\S*\s?', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenize the text
    tokens = word_tokenize(text)
    
    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]
    
    # Lemmatize each token
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    # Join lemmatized tokens back into a single string
    cleaned_text = ' '.join(lemmatized_tokens)
    
    return cleaned_text
