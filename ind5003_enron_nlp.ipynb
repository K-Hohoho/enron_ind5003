{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IND5003 Enron Project\n",
    "## Contents of this Jupyter Notebook\n",
    "### Dataset\n",
    "Dataset from Prof: \n",
    "(https://www.cs.cmu.edu/~./enron/)\n",
    "- Unstructured Dataset containing raw text in the form of emails\n",
    "**Make sure that the dataset 'maildir' is in the same directory as your project on your own system. Else this would not work**\n",
    "\n",
    "\n",
    "### Overall Steps to Tackle this Project\n",
    "1. Data Extraction\n",
    "2. Data Cleaning & Preprocessing\n",
    "3. Sender Frequency Analysis\n",
    "4. Visualisation\n",
    "    - Word Cloud & Bar Charts for the Top Senders (20%)\n",
    "    - Network Graph\n",
    "5. Unsupervised Techniques\n",
    "    - LDA Topic Modeling - Find out key topics from the top 20% of senders\n",
    "    - Temporal Analysis - Segment the emails by quarters. Look at the way communication changes over time.\n",
    "    - Hierarchical Clustering - Group senders based on communication patterns\n",
    "    - Anomaly Detection - Look for outliers and abnormal communication patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Data Extraction\n",
    "- Extract the emails from the unstructured raw folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries required for Section 1\n",
    "import os # Required for directory traversal\n",
    "import pandas as pd\n",
    "import email\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maildir path to the respective paths in your system \n",
    "# ! Note that maildir should be in the same directory as your project on your own system, would change if you are using windows\n",
    "maildir_path = '/Users/Dylan/Documents/IND5003/Projects/maildir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arnold-j', 'phanis-s', 'lavorato-j', 'stclair-c', 'townsend-j', 'forney-j', 'symes-k', 'reitmeyer-j', 'hyatt-k', 'steffes-j', 'kaminski-v', 'wolfe-j', 'mcconnell-m', 'skilling-j', 'zipper-a', 'shively-h', 'donoho-l', 'sanchez-m', 'delainey-d', 'germany-c', 'whalley-l', 'buy-r', 'harris-s', 'tholt-j', 'cash-m', 'sanders-r', '.DS_Store', 'staab-t', 'semperger-c', 'mccarty-d', 'mclaughlin-e', 'ring-a', 'stokley-c', 'hain-m', 'weldon-c', 'ring-r', 'farmer-d', 'sager-e', 'zufferli-j', 'ybarbo-p', 'watson-k', 'dasovich-j', 'arora-h', 'slinger-r', 'martin-t', 'storey-g', 'ruscitti-k', 'shankman-j', 'schwieger-j', 'perlingiere-d', 'saibi-e', 'griffith-j', 'meyers-a', 'grigsby-m', 'taylor-m', 'rapp-b', 'causholli-m', 'derrick-j', 'bass-e', 'south-s', 'salisbury-h', 'beck-s', 'tycholiz-b', 'shackleton-s', 'kitchen-l', 'merriss-s', 'blair-l', 'quenet-j', 'lokey-t', 'williams-j', 'panus-s', 'gang-l', 'hendrickson-s', 'schoolcraft-d', 'mann-k', 'kuykendall-t', 'allen-p', 'giron-d', 'lewis-a', 'jones-t', 'carson-m', 'stepenovitch-j', 'whitt-m', 'love-p', 'whalley-g', 'presto-k', 'scott-s', 'crandell-s', 'rodrique-r', 'white-s', 'motley-m', 'sturm-f', 'dean-c', 'keiser-k', 'shapiro-r', 'corman-s', 'pereira-s', 'campbell-l', 'richey-c', 'ward-k', 'dickson-s', 'rogers-b', 'nemec-g', 'hayslett-r', 'haedicke-m', 'mckay-j', 'gay-r', 'brawner-s', 'lucci-p', 'king-j', 'geaccone-t', 'guzman-m', 'mckay-b', 'hyvl-d', 'williams-w3', 'davis-d', 'thomas-p', 'linder-e', 'heard-m', 'hodge-j', 'pimenov-v', 'neal-s', 'fossum-d', 'baughman-d', 'smith-m', 'mims-thurston-p', 'bailey-s', 'fischer-m', 'dorland-c', 'ermis-f', 'may-l', 'platter-p', 'keavey-p', 'cuilla-m', 'parks-j', 'lokay-m', 'kean-s', 'quigley-d', 'horton-s', 'benson-r', 'solberg-g', 'badeer-r', 'lay-k', 'gilbertsmith-d', 'lenhart-m', 'swerzbin-m', 'hernandez-j', 'holst-k', 'maggi-m', 'donohoe-t', 'scholtes-d']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the directories in the maildir for sanity check\n",
    "maildir_list = os.listdir(maildir_path)\n",
    "print(maildir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_email(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             msg = BytesParser(policy=policy.default).parse(f)\n",
    "        \n",
    "#         # Extract fields from the email\n",
    "#         email_from = msg['From']\n",
    "#         email_to = msg['To']\n",
    "#         email_date = msg['Date']\n",
    "#         email_subject = msg['Subject']\n",
    "#         email_body = msg.get_body(preferencelist=('plain')).get_content() if msg.get_body(preferencelist=('plain')) else ''\n",
    "        \n",
    "#         return [email_from, email_to, email_date, email_subject, email_body]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error parsing file {file_path}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def batch_iterator(iterator, batch_size):\n",
    "#     \"\"\"Yield batches of specified size from an iterator.\"\"\"\n",
    "#     while True:\n",
    "#         batch = list(islice(iterator, batch_size))\n",
    "#         if not batch:\n",
    "#             break\n",
    "#         yield batch\n",
    "\n",
    "# def load_emails(maildir_path, batch_size=10, max_emails=50):\n",
    "#     email_data = []\n",
    "#     file_paths = []\n",
    "\n",
    "#     # Walk through the directory to collect file paths\n",
    "#     for root, dirs, files in os.walk(maildir_path):\n",
    "#         for file in files:\n",
    "#             if file == '.DS_Store' or file.startswith('.'):\n",
    "#                 continue  # Skip system files and hidden files\n",
    "#             file_paths.append(os.path.join(root, file))\n",
    "#             if len(file_paths) >= max_emails:\n",
    "#                 break\n",
    "#         if len(file_paths) >= max_emails:\n",
    "#             break\n",
    "\n",
    "#     # Process emails in batches\n",
    "#     for batch in batch_iterator(iter(file_paths), batch_size):\n",
    "#         batch_data = []\n",
    "#         for file_path in batch:\n",
    "#             result = parse_email(file_path)\n",
    "#             if result is not None:\n",
    "#                 batch_data.append(result)\n",
    "        \n",
    "#         # Append batch data to the main list\n",
    "#         email_data.extend(batch_data)\n",
    "\n",
    "#     # Create a DataFrame from the extracted data\n",
    "#     df = pd.DataFrame(email_data, columns=['From', 'To', 'Date', 'Subject', 'Body'])\n",
    "#     return df\n",
    "\n",
    "# # Load and parse emails\n",
    "# emails_df = load_emails(maildir_path, batch_size=10, max_emails=50)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(emails_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data into a Pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/sent_items/24.: 'ValueTerminal' object does not support item assignment\n",
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/_americas/netco_eol/83.: 'ValueTerminal' object does not support item assignment\n",
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/_americas/netco_eol/82.: 'ValueTerminal' object does not support item assignment\n",
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/_americas/esvl/87.: 'ValueTerminal' object does not support item assignment\n",
      "Error parsing file /Users/Dylan/Documents/IND5003/Projects/maildir/kitchen-l/_americas/netco_restart/3.: 'ValueTerminal' object does not support item assignment\n",
      "                        From  \\\n",
      "0            msagel@home.com   \n",
      "1    slafontaine@globalp.com   \n",
      "2    iceoperations@intcx.com   \n",
      "3  jeff.youngflesh@enron.com   \n",
      "4  caroline.abramo@enron.com   \n",
      "\n",
      "                                                  To  \\\n",
      "0                                  jarnold@enron.com   \n",
      "1                              john.arnold@enron.com   \n",
      "2  icehelpdesk@intcx.com, internalmarketing@intcx...   \n",
      "3  anthony.gilmore@enron.com, colleen.koenig@enro...   \n",
      "4                             mike.grigsby@enron.com   \n",
      "\n",
      "                              Date  \\\n",
      "0  Thu, 16 Nov 2000 09:30:00 -0800   \n",
      "1  Fri, 08 Dec 2000 05:05:00 -0800   \n",
      "2  Tue, 15 May 2001 09:43:00 -0700   \n",
      "3  Mon, 27 Nov 2000 01:49:00 -0800   \n",
      "4  Tue, 12 Dec 2000 09:33:00 -0800   \n",
      "\n",
      "                                             Subject  \\\n",
      "0                                             Status   \n",
      "1                                 re:summer inverses   \n",
      "2                      The WTI Bullet swap contracts   \n",
      "3  Invitation: EBS/GSS Meeting w/Bristol Babcock ...   \n",
      "4                                       Harvard Mgmt   \n",
      "\n",
      "                                                Body  \n",
      "0  John:\\n?\\nI'm not really sure what happened be...  \n",
      "1  i suck-hope youve made more money in natgas la...  \n",
      "2   Hi,\\n\\n\\n  Following the e-mail you have rece...  \n",
      "3  Conference Room TBD.  \\n\\nThis meeting will be...  \n",
      "4  Mike- I have their trader coming into the offi...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ! This is a very large dataset and will take a long time to run\n",
    "# ! DO NOT RUN THIS FOR FUN UNLESS YOU WANT YOUR COMPUTER TO CRASH\n",
    "def parse_email(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            msg = BytesParser(policy=policy.default).parse(f)\n",
    "        \n",
    "        # Extract fields from the email\n",
    "        email_from = msg['From']\n",
    "        email_to = msg['To']\n",
    "        email_date = msg['Date']\n",
    "        email_subject = msg['Subject']\n",
    "        email_body = msg.get_body(preferencelist=('plain')).get_content() if msg.get_body(preferencelist=('plain')) else ''\n",
    "        \n",
    "        return [email_from, email_to, email_date, email_subject, email_body]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_iterator(iterator, batch_size):\n",
    "    \"\"\"Yield batches of specified size from an iterator.\"\"\"\n",
    "    while True:\n",
    "        batch = list(islice(iterator, batch_size))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "def load_emails(maildir_path, batch_size=1000):\n",
    "    email_data = []\n",
    "    file_paths = []\n",
    "\n",
    "    # Walk through the directory to collect file paths\n",
    "    for root, dirs, files in os.walk(maildir_path):\n",
    "        for file in files:\n",
    "            if file == '.DS_Store' or file.startswith('.'):\n",
    "                continue  # Skip system files and hidden files\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    # Process emails in batches\n",
    "    for batch in batch_iterator(iter(file_paths), batch_size):\n",
    "        batch_data = []\n",
    "        for file_path in batch:\n",
    "            result = parse_email(file_path)\n",
    "            if result is not None:\n",
    "                batch_data.append(result)\n",
    "        \n",
    "        # Append batch data to the main list\n",
    "        email_data.extend(batch_data)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(email_data, columns=['From', 'To', 'Date', 'Subject', 'Body'])\n",
    "    return df\n",
    "\n",
    "# Load and parse emails\n",
    "emails_df = load_emails(maildir_path, batch_size=1000)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(emails_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emails_df to a CSV file\n",
    "# Save the DataFrame as a CSV file in the specified directory\n",
    "\n",
    "#emails_df.to_csv('/Users/Dylan/Documents/IND5003/Projects/emails_uncleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        From  \\\n",
      "0            msagel@home.com   \n",
      "1    slafontaine@globalp.com   \n",
      "2    iceoperations@intcx.com   \n",
      "3  jeff.youngflesh@enron.com   \n",
      "4  caroline.abramo@enron.com   \n",
      "\n",
      "                                                  To  \\\n",
      "0                                  jarnold@enron.com   \n",
      "1                              john.arnold@enron.com   \n",
      "2  icehelpdesk@intcx.com, internalmarketing@intcx...   \n",
      "3  anthony.gilmore@enron.com, colleen.koenig@enro...   \n",
      "4                             mike.grigsby@enron.com   \n",
      "\n",
      "                              Date  \\\n",
      "0  Thu, 16 Nov 2000 09:30:00 -0800   \n",
      "1  Fri, 08 Dec 2000 05:05:00 -0800   \n",
      "2  Tue, 15 May 2001 09:43:00 -0700   \n",
      "3  Mon, 27 Nov 2000 01:49:00 -0800   \n",
      "4  Tue, 12 Dec 2000 09:33:00 -0800   \n",
      "\n",
      "                                             Subject  \\\n",
      "0                                             Status   \n",
      "1                                 re:summer inverses   \n",
      "2                      The WTI Bullet swap contracts   \n",
      "3  Invitation: EBS/GSS Meeting w/Bristol Babcock ...   \n",
      "4                                       Harvard Mgmt   \n",
      "\n",
      "                                                Body  \n",
      "0  John:\\n?\\nI'm not really sure what happened be...  \n",
      "1  i suck-hope youve made more money in natgas la...  \n",
      "2   Hi,\\n\\n\\n  Following the e-mail you have rece...  \n",
      "3  Conference Room TBD.  \\n\\nThis meeting will be...  \n",
      "4  Mike- I have their trader coming into the offi...  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file back into a DataFrame\n",
    "\n",
    "# PLEASE CHANGE IT TO YOUR OWN DIRECTORY IN YOUR OWN SYSTEM \n",
    "\n",
    "enron_uncleaned_emails = pd.read_csv('/Users/Dylan/Documents/IND5003/Projects/enron_emails_uncleaned.csv')\n",
    "print(enron_uncleaned_emails.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Preprocessing\n",
    "### Start with the Cleaning\n",
    "* Check for any nulls\n",
    "* Drop the missing values\n",
    "* Remove the duplicates\n",
    "* Format the dates \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Column  Missing Values\n",
      "0     From               0\n",
      "1       To           21847\n",
      "2     Date               0\n",
      "3  Subject           19187\n",
      "4     Body               0\n"
     ]
    }
   ],
   "source": [
    "# Check for Nulls in Each Column\n",
    "missing_values = enron_uncleaned_emails.isnull().sum()\n",
    "missing_values_df = pd.DataFrame({'Column': missing_values.index, 'Missing Values': missing_values.values})\n",
    "print(missing_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Date</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>517396</td>\n",
       "      <td>495549</td>\n",
       "      <td>517396</td>\n",
       "      <td>498209</td>\n",
       "      <td>517396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20326</td>\n",
       "      <td>58556</td>\n",
       "      <td>224119</td>\n",
       "      <td>159286</td>\n",
       "      <td>249020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>kay.mann@enron.com</td>\n",
       "      <td>pete.davis@enron.com</td>\n",
       "      <td>Wed, 27 Jun 2001 16:02:00 -0700</td>\n",
       "      <td>RE:</td>\n",
       "      <td>As you know, Enron Net Works (ENW) and Enron G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>16735</td>\n",
       "      <td>9155</td>\n",
       "      <td>1118</td>\n",
       "      <td>6477</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      From                    To  \\\n",
       "count               517396                495549   \n",
       "unique               20326                 58556   \n",
       "top     kay.mann@enron.com  pete.davis@enron.com   \n",
       "freq                 16735                  9155   \n",
       "\n",
       "                                   Date Subject  \\\n",
       "count                            517396  498209   \n",
       "unique                           224119  159286   \n",
       "top     Wed, 27 Jun 2001 16:02:00 -0700     RE:   \n",
       "freq                               1118    6477   \n",
       "\n",
       "                                                     Body  \n",
       "count                                              517396  \n",
       "unique                                             249020  \n",
       "top     As you know, Enron Net Works (ENW) and Enron G...  \n",
       "freq                                                  112  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description of the DataFrame\n",
    "enron_uncleaned_emails.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out the missing values with empty strings\n",
    "enron_cleaned_emails = enron_uncleaned_emails.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Column  Missing Values\n",
      "0     From               0\n",
      "1       To               0\n",
      "2     Date               0\n",
      "3  Subject               0\n",
      "4     Body               0\n"
     ]
    }
   ],
   "source": [
    "# Post cleaning Check\n",
    "missing_values_check = enron_cleaned_emails.isnull().sum()\n",
    "missing_values_df_check = pd.DataFrame({'Column': missing_values_check.index, 'Missing Values': missing_values_check.values})\n",
    "print(missing_values_df_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Date</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>517396</td>\n",
       "      <td>517396</td>\n",
       "      <td>517396</td>\n",
       "      <td>517396</td>\n",
       "      <td>517396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>20326</td>\n",
       "      <td>58557</td>\n",
       "      <td>224119</td>\n",
       "      <td>159287</td>\n",
       "      <td>249020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>kay.mann@enron.com</td>\n",
       "      <td></td>\n",
       "      <td>Wed, 27 Jun 2001 16:02:00 -0700</td>\n",
       "      <td></td>\n",
       "      <td>As you know, Enron Net Works (ENW) and Enron G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>16735</td>\n",
       "      <td>21847</td>\n",
       "      <td>1118</td>\n",
       "      <td>19187</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      From      To                             Date Subject  \\\n",
       "count               517396  517396                           517396  517396   \n",
       "unique               20326   58557                           224119  159287   \n",
       "top     kay.mann@enron.com          Wed, 27 Jun 2001 16:02:00 -0700           \n",
       "freq                 16735   21847                             1118   19187   \n",
       "\n",
       "                                                     Body  \n",
       "count                                              517396  \n",
       "unique                                             249020  \n",
       "top     As you know, Enron Net Works (ENW) and Enron G...  \n",
       "freq                                                  112  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the cleaned DataFrame\n",
    "enron_cleaned_emails.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the columns subject and body\n",
    "# Format Date and Time to correct format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ind5003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')  
nltk.download('stopwords')  
#Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Define the cleaning and lemmatizing function
def clean_and_lemmatize(text):
    # Convert text to lowercase
    text = text.lower()
    
    # Remove email addresses, URLs, numbers, and special characters
    text = re.sub(r'\S*@\S*\s?', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenize the text
    tokens = word_tokenize(text)
    
    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]
    
    # Lemmatize each token
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    # Join lemmatized tokens back into a single string
    cleaned_text = ' '.join(lemmatized_tokens)
    
    return cleaned_text
